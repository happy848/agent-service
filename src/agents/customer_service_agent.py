# æµ‹è¯•å®¢æœæœºå™¨äººå·¥ä½œæµæ–‡æ¡£ docs/customer_service_agent.md
# æœºå™¨äººå·¥ä½œæµï¼š
# 1. ç”¨æˆ·èƒŒæ™¯ä¿¡æ¯æŸ¥è¯¢å’Œé—®é¢˜åˆ†ç±»å¹¶è¡Œæ‰§è¡Œ
# 2. æ¨ç†å›ç­”ã€è‡ªæˆ‘æ£€æŸ¥ã€æ‹Ÿäººå›å¤ä¸²è¡Œæ‰§è¡Œ
# 3. é—®é¢˜åˆ†ç±»ç»†åŒ–
# 4. æ‹ŸäººåŒ–è¯­è¨€é£æ ¼

from datetime import datetime
from typing import Literal, List, Dict, Any, Optional, Annotated
import re
import operator
import logging

from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.messages import AIMessage, SystemMessage, HumanMessage
from langchain_core.runnables import RunnableConfig, RunnableLambda, RunnableSerializable
from langchain_core.tools import tool
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import END, MessagesState, StateGraph, START

from core import get_model, settings
from schema import ChatMessage
from tools.user_info import get_user_summary, get_user_orders, get_user_parcels

# ä½¿ç”¨å…¨å±€loggingï¼Œä¸éœ€è¦å•ç‹¬çš„loggeré…ç½®

# Define dictionary merge function for parallel execution
def merge_dicts(left: Dict[str, Any], right: Dict[str, Any]) -> Dict[str, Any]:
    """Merge two dictionaries, with right dict taking precedence for overlapping keys."""
    if left is None:
        return right
    if right is None:
        return left
    result = left.copy()
    result.update(right)
    return result

class CustomerServiceState(MessagesState, total=False):
    """State for customer service agent."""
    # ä½¿ç”¨è‡ªå®šä¹‰çš„merge_dictså‡½æ•°ä½œä¸ºreducerï¼Œæ”¯æŒå¹¶è¡Œæ‰§è¡Œæ—¶çš„çŠ¶æ€åˆå¹¶
    categories: Annotated[Dict[str, Any], merge_dicts]
    userInfo: Optional[Dict[str, Any]]
    userToken: Optional[str]
    backgroundInfo: Annotated[Dict[str, Any], merge_dicts]  # ç”¨æˆ·èƒŒæ™¯ä¿¡æ¯
    reasoning_response: Optional[str]  # æ¨ç†å›ç­”
    humanized_response: Optional[str]  # æ‹Ÿäººå›å¤
    self_check_passed: Optional[bool]  # è‡ªæˆ‘æ£€æŸ¥ç»“æœ
    check_count: Optional[int]  # æ£€æŸ¥æ¬¡æ•°


async def categorize_message(message: str) -> Dict[str, Any]:
    """Categorize the customer message to determine intent."""
    # æ ¹æ®æ–‡æ¡£è¦æ±‚çš„5ä¸ªå…·ä½“åˆ†ç±»
    categories = {
        "new_user_onboarding": ["æ³¨å†Œ", "ç™»å½•", "æ€ä¹ˆä½¿ç”¨", "å¦‚ä½•ä½¿ç”¨", "æ–°ç”¨æˆ·", "ç¬¬ä¸€æ¬¡", "æ•™ç¨‹", "guide", "register", "login", "how to use", "first time", "tutorial"],
        "payment_issues": ["æ”¯ä»˜", "ä»˜æ¬¾", "å……å€¼", "ä½™é¢", "æ‰‹ç»­è´¹", "è½¬è´¦", "revolut", "wise", "payment", "pay", "top up", "balance", "fee", "transfer"],
        "order_issues": ["è®¢å•", "å¤šä¹…åˆ°ä»“åº“", "å¤šä¹…åˆ°å®¶", "è´¨æ£€ç…§ç‰‡", "é€€è´§", "order", "warehouse", "delivery time", "quality check", "return", "refund"],
        "parcel_issues": ["åŒ…è£¹", "è¿è´¹", "åŒ…è£¹çŠ¶æ€", "è¢«æ²¡æ”¶", "tracking", "parcel", "shipping cost", "parcel status", "confiscated"],
        "other_issues": ["å¹³å°ä»‹ç»", "å¹³å°æ”¿ç­–", "å¹³å°æ´»åŠ¨", "å…¶ä»–", "ä»‹ç»", "æ”¿ç­–", "æ´»åŠ¨", "platform", "policy", "activity", "other"]
    }
    
    message_lower = message.lower()
    detected_categories = []
    
    # é¦–å…ˆå°è¯•å…³é”®è¯åŒ¹é…
    for category, keywords in categories.items():
        if any(keyword in message_lower for keyword in keywords):
            detected_categories.append(category)
    
    # å¦‚æœå…³é”®è¯åŒ¹é…æˆåŠŸï¼Œç›´æ¥è¿”å›ç»“æœ
    if detected_categories:
        return {
            "categories": detected_categories,
            "confidence": 0.9,
            "method": "keyword_matching"
        }
    
    # å¦‚æœæ²¡æœ‰å…³é”®è¯å‘½ä¸­ï¼Œä½¿ç”¨å¤§æ¨¡å‹æ¨ç†
    try:
        model = get_model(settings.DEFAULT_MODEL)
        
        classification_prompt = f"""è¯·åˆ†æä»¥ä¸‹å®¢æˆ·æ¶ˆæ¯ï¼Œå°†å…¶åˆ†ç±»åˆ°æœ€åˆé€‚çš„ç±»åˆ«ä¸­ã€‚
            å®¢æˆ·æ¶ˆæ¯ï¼š{message}
            å¯é€‰åˆ†ç±»ï¼š
            1. new_user_onboarding - æ–°ç”¨æˆ·å¼•å¯¼ï¼ˆæ³¨å†Œã€ç™»å½•ã€å¦‚ä½•ä½¿ç”¨å¹³å°ã€æ•™ç¨‹ç­‰ï¼‰
            2. payment_issues - æ”¯ä»˜é—®é¢˜ï¼ˆæ”¯ä»˜ã€å……å€¼ã€ä½™é¢ã€æ‰‹ç»­è´¹ã€è½¬è´¦ç­‰ï¼‰
            3. order_issues - è®¢å•é—®é¢˜ï¼ˆè®¢å•çŠ¶æ€ã€åˆ°è´§æ—¶é—´ã€è´¨æ£€ã€é€€è´§ç­‰ï¼‰
            4. parcel_issues - åŒ…è£¹é—®é¢˜ï¼ˆåŒ…è£¹çŠ¶æ€ã€è¿è´¹ã€ç‰©æµã€è¢«æ²¡æ”¶ç­‰ï¼‰
            5. other_issues - å…¶ä»–é—®é¢˜ï¼ˆå¹³å°ä»‹ç»ã€æ”¿ç­–ã€æ´»åŠ¨ç­‰ï¼‰
            è¯·åªè¿”å›åˆ†ç±»åç§°ï¼Œä¸è¦åŒ…å«å…¶ä»–å†…å®¹ã€‚å¦‚æœæ¶ˆæ¯æ¶‰åŠå¤šä¸ªç±»åˆ«ï¼Œè¯·é€‰æ‹©æœ€ä¸»è¦çš„ä¸€ä¸ªã€‚
            åˆ†ç±»ç»“æœï¼š"""

        messages = [
            SystemMessage(content=classification_prompt),
            HumanMessage(content="è¯·è¿›è¡Œåˆ†ç±»")
        ]
        
        response = await model.ainvoke(messages)
        llm_category = response.content.strip().lower()
        
        # éªŒè¯LLMè¿”å›çš„åˆ†ç±»æ˜¯å¦æœ‰æ•ˆ
        valid_categories = list(categories.keys())
        if llm_category in valid_categories:
            detected_categories = [llm_category]
            confidence = 0.7  # LLMæ¨ç†çš„ç½®ä¿¡åº¦ç¨ä½
        else:
            # å¦‚æœLLMè¿”å›çš„åˆ†ç±»æ— æ•ˆï¼Œé»˜è®¤ä¸ºother_issues
            detected_categories = ["other_issues"]
            confidence = 0.5
            
        return {
            "categories": detected_categories,
            "confidence": confidence,
            "method": "llm_inference",
            "llm_raw_response": llm_category
        }
        
    except Exception as e:
        logging.warning(f"LLM inference failed for message categorization: {e}")
        # å¦‚æœLLMæ¨ç†å¤±è´¥ï¼Œé»˜è®¤ä¸ºother_issues
        return {
            "categories": ["other_issues"],
            "confidence": 0.3,
            "method": "fallback"
        }

async def categorize_customer_message(state: CustomerServiceState, config: RunnableConfig) -> CustomerServiceState:
    """Categorize the customer message to determine intent."""
    last_message = state["messages"][-1]
    if not isinstance(last_message, HumanMessage):
        return {"categories": {}}
        
    categories = await categorize_message(last_message.content)
    return {"categories": categories}

async def get_user_information(state: CustomerServiceState, config: RunnableConfig) -> CustomerServiceState:
    """Get user information using userToken from config."""
    try:
        # ä»configä¸­è·å–user_token
        userToken = config.get("configurable", {}).get("userToken")
        logging.info(f"User token: {userToken}")
        logging.info(f"User configurable: {config.get('configurable', {})}")
        logging.info(f"User config: {config}")
        if not userToken:
            return {
                "userInfo": None, 
                "userToken": None, 
                "backgroundInfo": {
                    "userInfo": None,
                    "orders": [],
                    "parcels": []
                }
            }
        
        # è·å–ç”¨æˆ·ä¿¡æ¯
        userInfo = await get_user_summary(userToken)
        
        logging.info(f"User token: {userToken}")
        logging.info(f"User info: {userInfo}")
        
        # å¹¶å‘è·å–ç”¨æˆ·è®¢å•å’ŒåŒ…è£¹ä¿¡æ¯
        backgroundInfo = {
        
            "userInfo": userInfo,
            "orders": [],
            "parcels": []
        }
        
        try:
            orders = await get_user_orders(userToken)
            backgroundInfo["orders"] = [order.dict() for order in orders]
        except Exception as e:
            logging.warning(f"Failed to get user orders: {e}")
            
        try:
            parcels = await get_user_parcels(userToken)
            backgroundInfo["parcels"] = [parcel.dict() for parcel in parcels]
        except Exception as e:
            logging.warning(f"Failed to get user parcels: {e}")
        
        return {
            "userInfo": userInfo, 
            "userToken": userToken,
            "backgroundInfo": backgroundInfo
        }
        
    except Exception as e:
        # å¦‚æœè·å–ç”¨æˆ·ä¿¡æ¯å¤±è´¥ï¼Œè®°å½•é”™è¯¯ä½†ç»§ç»­å¤„ç†
        logging.error(f"Failed to get user info: {e}")
        return {
            "userInfo": None, 
            "userToken": userToken, 
            "backgroundInfo": {
                "userInfo": None,
                "orders": [],
                "parcels": []
            }
        }

async def reasoning_response(state: CustomerServiceState, config: RunnableConfig) -> CustomerServiceState:
    """æ¨ç†å›ç­”ï¼šåŸºäºèƒŒæ™¯ä¿¡æ¯å’Œé—®é¢˜åˆ†ç±»è¿›è¡Œæ·±åº¦æ¨ç†"""
    m = get_model(config["configurable"].get("model", settings.DEFAULT_MODEL))
    
    # æ„å»ºæ¨ç†æç¤º
    user_message = state["messages"][-1].content if state["messages"] else ""
    categories = state.get("categories", {}).get("categories", [])
    backgroundInfo = state.get("backgroundInfo", {})
    
    current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    reasoning_prompt = f"""åŸºäºä»¥ä¸‹ä¿¡æ¯è¿›è¡Œæ·±åº¦æ¨ç†ï¼Œç”Ÿæˆä¸“ä¸šã€å‡†ç¡®çš„å›ç­”ï¼š
å½“å‰æ—¶é—´ï¼š{current_time}
ç”¨æˆ·é—®é¢˜ï¼š{user_message}
é—®é¢˜åˆ†ç±»ï¼š{categories}
ç”¨æˆ·èƒŒæ™¯ä¿¡æ¯ï¼š{backgroundInfo}

è¯·æ ¹æ®é—®é¢˜åˆ†ç±»æä¾›ç›¸åº”çš„ä¸“ä¸šå›ç­”ï¼š

1. æ–°ç”¨æˆ·onboardingï¼šè¯¦ç»†è¯´æ˜æ³¨å†Œæµç¨‹ã€å¹³å°ä½¿ç”¨æ–¹æ³•ã€è´­ç‰©è½¦åˆ¶ä½œã€æ”¯ä»˜æµç¨‹
2. æ”¯ä»˜é—®é¢˜ï¼šè§£é‡Šæ”¯ä»˜æ¸ é“ã€æ‰‹ç»­è´¹ã€å……å€¼æµç¨‹
3. è®¢å•é—®é¢˜ï¼šè¯´æ˜è®¢å•å¤„ç†æ—¶é—´ã€è´¨æ£€æµç¨‹ã€é€€è´§æ”¿ç­–
4. åŒ…è£¹é—®é¢˜ï¼šè§£é‡Šè¿è´¹æ„æˆã€é…é€æ—¶é—´ã€åŒ…è£¹çŠ¶æ€æŸ¥è¯¢ã€å¼‚å¸¸å¤„ç†
5. å…¶ä»–é—®é¢˜ï¼šæä¾›å¹³å°ä»‹ç»ã€æ”¿ç­–è¯´æ˜ã€æ´»åŠ¨ä¿¡æ¯

è¦æ±‚ï¼š
- å›ç­”è¦å‡†ç¡®ã€ä¸“ä¸šã€å®Œæ•´
- åŸºäºç”¨æˆ·èƒŒæ™¯ä¿¡æ¯æä¾›ä¸ªæ€§åŒ–å»ºè®®
- æä¾›å…·ä½“çš„æ“ä½œæ­¥éª¤å’Œè§£å†³æ–¹æ¡ˆ
"""
    
    messages = [
        SystemMessage(content=reasoning_prompt),
        HumanMessage(content=user_message)
    ]
    
    response = await m.ainvoke(messages, config)
    
    # é‡ç½®æ£€æŸ¥æ¬¡æ•°ï¼Œå¼€å§‹æ–°çš„æ¨ç†
    return {"reasoning_response": response.content}

async def self_check_response(state: CustomerServiceState, config: RunnableConfig) -> CustomerServiceState:
    """å›ç­”è‡ªæˆ‘æ£€æŸ¥ï¼šæ£€æŸ¥å›ç­”æ˜¯å¦å‡†ç¡®ã€å®Œæ•´ã€æ— å¹»è§‰ï¼Œæœ€å¤šæ£€æŸ¥2æ¬¡"""
    check_count = state.get("check_count", 0)
    if check_count >= 1:
        logging.info(f"Self check passed after {check_count} attempts (max limit reached)")
        return {"self_check_passed": True, "check_count": check_count}
    
    m = get_model(config["configurable"].get("model", settings.DEFAULT_MODEL))
    
    reasoning_response = state.get("reasoning_response", "")
    user_message = state["messages"][-1].content if state["messages"] else ""
    
    # è·å–å½“å‰æ£€æŸ¥æ¬¡æ•°ï¼Œé»˜è®¤ä¸º0
    
    
    check_prompt = f"""è¯·å¯¹ä»¥ä¸‹å®¢æœå›ç­”è¿›è¡Œè‡ªæˆ‘æ£€æŸ¥ï¼š

ç”¨æˆ·é—®é¢˜ï¼š{user_message}
å®¢æœå›ç­”ï¼š{reasoning_response}

æ£€æŸ¥æ ‡å‡†ï¼š
1. æ˜¯å¦å‡†ç¡®å›ç­”äº†ç”¨æˆ·çš„é—®é¢˜,èšç„¦è§£å†³ç”¨æˆ·å½“å‰é—®é¢˜ï¼Œä¸è¦æ‰©å±•å…¶ä»–é—®é¢˜
2. æ˜¯å¦æœ‰äº‹å®é”™è¯¯æˆ–å¹»è§‰ï¼Œä¸è¦ç¼–é€ äº‹å®
3. æ˜¯å¦è¿èƒŒå¸¸è¯†
4. ä¸éœ€è¦æ£€æŸ¥ç”¨æˆ·èº«ä»½ï¼Œä¸è¦è¯¢é—®ç”¨æˆ·èº«ä»½ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨è·å–ç”¨æˆ·èº«ä»½å’Œè®¢å•ä¿¡æ¯ç»™ä¹‹å‰çš„æ¨¡å‹

è¯·ç»™å‡ºæ£€æŸ¥ç»“æœï¼š
- å¦‚æœæ£€æŸ¥é€šè¿‡ï¼Œå›å¤"PASS"
- å¦‚æœæ£€æŸ¥ä¸é€šè¿‡ï¼Œå›å¤"FAIL"å¹¶è¯´æ˜åŸå› 

æ£€æŸ¥ç»“æœï¼š"""
    
    messages = [
        SystemMessage(content=check_prompt),
        HumanMessage(content="è¯·è¿›è¡Œè‡ªæˆ‘æ£€æŸ¥")
    ]
    
    response = await m.ainvoke(messages, config)
    check_result = response.content.strip()
    
    if "PASS" in check_result.upper():
        return {"self_check_passed": True}
    else:
        # æ£€æŸ¥ä¸é€šè¿‡ï¼Œå¢åŠ æ£€æŸ¥æ¬¡æ•°
        new_check_count = check_count + 1
        logging.warning(f"Self check failed (attempt {new_check_count}/2): {check_result}")
        
        # å¦‚æœè¾¾åˆ°2æ¬¡æ£€æŸ¥ï¼Œç›´æ¥é€šè¿‡
        if new_check_count >= 2:
            logging.info(f"Self check passed after {new_check_count} attempts (max limit reached)")
            return {"self_check_passed": True, "check_count": new_check_count}
        else:
            return {"self_check_passed": False, "check_feedback": check_result, "check_count": new_check_count}

async def humanize_response(state: CustomerServiceState, config: RunnableConfig) -> CustomerServiceState:
    """æ‹Ÿäººå›å¤ï¼šå°†ä¸“ä¸šå›ç­”è½¬æ¢ä¸ºè‡ªç„¶ã€å£è¯­åŒ–çš„è¯­è¨€"""
    m = get_model(config["configurable"].get("model", settings.DEFAULT_MODEL))
    
    reasoning_response = state.get("reasoning_response", "")
    user_message = state["messages"][-1].content if state["messages"] else ""
    backgroundInfo = state.get("backgroundInfo", {})
    
    humanize_prompt = f"""è¯·å°†ä»¥ä¸‹ä¸“ä¸šå›ç­”è½¬æ¢ä¸ºè‡ªç„¶ã€å£è¯­åŒ–çš„æ‹Ÿäººå›å¤ï¼š

åŸå§‹å›ç­”ï¼š{reasoning_response}
ç”¨æˆ·é—®é¢˜ï¼š{user_message}

æ‹ŸäººåŒ–è¦æ±‚ï¼š
1. ä½¿ç”¨è‡ªç„¶ã€å£è¯­åŒ–çš„è¯­è¨€ï¼Œé¿å…ç”Ÿç¡¬ã€æœºæ¢°çš„è¡¨è¿°
2. é¿å…è¿‡äºä¸“ä¸šçš„æœ¯è¯­ï¼Œä½¿ç”¨æ—¥å¸¸äº¤æµä¹ æƒ¯çš„è¡¨è¾¾
3. æ ¹æ®é—®é¢˜æ€§è´¨èå…¥ç§¯æçš„æƒ…æ„Ÿå…ƒç´ ï¼š
   - è§£å†³é—®é¢˜åè¡¨è¾¾"å¾ˆé«˜å…´èƒ½ä¸ºæ‚¨è§£å†³è¿™ä¸ªé—®é¢˜"
   - ç”¨æˆ·é‡åˆ°å›°æ‰°æ—¶è¡¨è¾¾"éå¸¸ç†è§£æ‚¨çš„å¿ƒæƒ…"
4. é€‚å½“æ·»åŠ è¯­æ°”è¯ï¼ˆå¦‚"å‘¢""å•¦""å“¦"ï¼‰å’Œè¡¨æƒ…ç¬¦å·ï¼ˆğŸ˜Šã€ğŸ‘ç­‰ï¼‰
5. ä½¿ç”¨ç¬¬ä¸€äººç§°ï¼ˆæˆ‘ã€æˆ‘ä»¬ï¼‰å’Œç¬¬äºŒäººç§°ï¼ˆæ‚¨ï¼‰å¢å¼ºäº’åŠ¨æ„Ÿ
6. æ ¹æ®ç”¨æˆ·èƒŒæ™¯ä¿¡æ¯è¿›è¡Œä¸ªæ€§åŒ–è°ƒæ•´
7. åœ¨ç»“å°¾æ·»åŠ å‹å¥½çš„äº’åŠ¨è¯­å¥ï¼Œå¦‚"å¦‚æœæ‚¨è¿˜æœ‰å…¶ä»–ç–‘é—®ï¼Œæ¬¢è¿éšæ—¶å‘æˆ‘å’¨è¯¢å“¦ï½"

æ³¨æ„ï¼š
- ä¿æŒä¸“ä¸šæ€§ï¼Œé¿å…è¿‡åº¦ä½¿ç”¨è¡¨æƒ…ç¬¦å·
- ç¡®ä¿ä¿¡æ¯å‡†ç¡®æ€§å’Œå®Œæ•´æ€§
- ä½¿ç”¨"\n\n"åˆ†éš”é•¿æ¶ˆæ¯

æ‹ŸäººåŒ–å›å¤ï¼š"""
    
    messages = [
        SystemMessage(content=humanize_prompt),
        HumanMessage(content="è¯·è¿›è¡Œæ‹ŸäººåŒ–å¤„ç†")
    ]
    
    response = await m.ainvoke(messages, config)
    return {"humanized_response": response.content}

async def final_response(state: CustomerServiceState, config: RunnableConfig) -> CustomerServiceState:
    """æœ€ç»ˆå›å¤ï¼šå°†æ‹ŸäººåŒ–å›å¤æ·»åŠ åˆ°æ¶ˆæ¯ä¸­"""
    humanized_response = state.get("humanized_response", "")
    if humanized_response:
        return {"messages": [AIMessage(content=humanized_response)]}
    else:
        # å¦‚æœæ²¡æœ‰æ‹ŸäººåŒ–å›å¤ï¼Œä½¿ç”¨æ¨ç†å›å¤
        reasoning_response = state.get("reasoning_response", "")
        return {"messages": [AIMessage(content=reasoning_response)]}

# Define the graph
agent = StateGraph(CustomerServiceState)

# Add nodes
agent.add_node("get_user_info", get_user_information)
agent.add_node("category_analyzer", categorize_customer_message)
agent.add_node("reasoning", reasoning_response)
agent.add_node("self_check", self_check_response)
agent.add_node("humanize", humanize_response)
agent.add_node("final_response", final_response)

# å¹¶è¡Œæ‰§è¡Œï¼šç”¨æˆ·èƒŒæ™¯ä¿¡æ¯æŸ¥è¯¢å’Œé—®é¢˜åˆ†ç±»
# ä»STARTå¼€å§‹ï¼ŒåŒæ—¶æ‰§è¡Œget_user_infoå’Œcategory_analyzer
agent.add_edge(START, "get_user_info")
agent.add_edge(START, "category_analyzer")

# ä¸²è¡Œæ‰§è¡Œï¼šæ¨ç†å›ç­”ã€è‡ªæˆ‘æ£€æŸ¥ã€æ‹Ÿäººå›å¤
agent.add_edge("get_user_info", "reasoning")
agent.add_edge("category_analyzer", "reasoning")
agent.add_edge("reasoning", "self_check")

# æ¡ä»¶åˆ†æ”¯ï¼šå¦‚æœè‡ªæˆ‘æ£€æŸ¥é€šè¿‡ï¼Œç»§ç»­æ‹ŸäººåŒ–ï¼›å¦åˆ™é‡æ–°æ¨ç†
def should_continue(state: CustomerServiceState) -> str:
    """å†³å®šæ˜¯å¦ç»§ç»­åˆ°æ‹ŸäººåŒ–æ­¥éª¤"""
    if state.get("self_check_passed", False):
        return "humanize"
    else:
        return "reasoning"  # é‡æ–°æ¨ç†

agent.add_conditional_edges("self_check", should_continue)

agent.add_edge("humanize", "final_response")
agent.add_edge("final_response", END)

# Compile the agent with parallel execution
customer_service_agent = agent.compile(
    checkpointer=MemorySaver(),
)
customer_service_agent.name = "customer-service-agent"


# ä½¿ç”¨ç¤ºä¾‹
async def example_customer_service_usage():
    """å®¢æœæ™ºèƒ½ä½“ä½¿ç”¨ç¤ºä¾‹"""
    
    # æ¨¡æ‹Ÿç”¨æˆ·æ¶ˆæ¯
    user_message = "Hi, I want to check my order status"
    userToken = "example_user_token_123"
    
    # åˆ›å»ºåˆå§‹çŠ¶æ€
    initial_state = {
        "messages": [HumanMessage(content=user_message)]
    }
    
    # é…ç½®ï¼ŒåŒ…å«user_token
    config = {
        "configurable": {
            "userToken": userToken,
            "model": settings.DEFAULT_MODEL
        }
    }
    
    try:
        # è¿è¡Œæ™ºèƒ½ä½“
        result = await customer_service_agent.ainvoke(initial_state, config)
        
        # è·å–AIå›å¤
        ai_messages = [msg for msg in result["messages"] if isinstance(msg, AIMessage)]
        
        logging.info("Customer Service Agent Response:")
        for msg in ai_messages:
            logging.info(f"AI: {msg.content}")
            
        # æ‰“å°ç”¨æˆ·ä¿¡æ¯ï¼ˆå¦‚æœè·å–æˆåŠŸï¼‰
        if result.get("userInfo"):
            userInfo = result["userInfo"]
            logging.info(f"\nUser Info Retrieved:")
            logging.info(f"- Email: {userInfo.email}")
            logging.info(f"- VIP Level: {userInfo.vip_level}")
            logging.info(f"- Balance: {userInfo.balance_cny} CNY")
            
        # æ‰“å°å¹¶è¡Œæ‰§è¡Œçš„ç»“æœ
        logging.info(f"\nParallel Execution Results:")
        logging.info(f"- Categories: {result.get('categories', {})}")
        logging.info(f"- Background Info: {result.get('backgroundInfo', {})}")
            
    except Exception as e:
        logging.info(f"Error running customer service agent: {e}")

# æµ‹è¯•å¹¶è¡Œæ‰§è¡Œ
async def test_parallel_execution():
    """æµ‹è¯•å¹¶è¡Œæ‰§è¡Œæ˜¯å¦æ­£å¸¸å·¥ä½œ"""
    logging.info("Testing parallel execution of get_user_info and category_analyzer...")
    
    # æ¨¡æ‹Ÿç”¨æˆ·æ¶ˆæ¯
    user_message = "How much is shipping to Germany?"
    userToken = "test_user_token"
    
    # åˆ›å»ºåˆå§‹çŠ¶æ€
    initial_state = {
        "messages": [HumanMessage(content=user_message)]
    }
    
    # é…ç½®
    config = {
        "configurable": {
            "userToken": userToken,
            "model": settings.DEFAULT_MODEL
        }
    }
    
    try:
        # è¿è¡Œæ™ºèƒ½ä½“
        result = await customer_service_agent.ainvoke(initial_state, config)
        
        logging.info("âœ… Parallel execution test completed!")
        logging.info(f"Categories: {result.get('categories', {})}")
        logging.info(f"User Info: {result.get('userInfo', 'Not retrieved')}")
        logging.info(f"Background Info: {result.get('backgroundInfo', {})}")
        
        return True
        
    except Exception as e:
        logging.info(f"âŒ Parallel execution test failed: {e}")
        return False
